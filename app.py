import streamlit as st
import requests
import xmltodict 
import urllib.parse 
import re  # HTML íƒœê·¸ ì œê±°ë¥¼ ìœ„í•œ ëª¨ë“ˆ

st.set_page_config(page_title="ê³ êµí•™ì ì œ ìë£Œ íƒìƒ‰", layout="centered")
st.title("ğŸ“š ì—ë“€ë„· ê³µì‹ ìë£Œ ê²€ìƒ‰ê¸°")
st.write("ê¶ê¸ˆí•œ ê³¼ëª©/í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ê³  ê³µì‹ ìë£Œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# 1. API í‚¤ ë° ë„ë©”ì¸ ì„¤ì • (Streamlit Secretsì—ì„œ ê°€ì ¸ì˜´)
try:
    SNO_KEY = st.secrets["KERIS_SNO_KEY"] 
    SVC_DOMAIN = st.secrets.get("SVC_DOMAIN", "highschool-guide.streamlit.app") 
except KeyError:
    st.error("ğŸ”‘ KERIS_SNO_KEY ë˜ëŠ” SVC_DOMAINì´ Secretsì— ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()


# --- ì—ë“€ë„· API í˜¸ì¶œ í•¨ìˆ˜ ---
def search_keris_contents(query, collection_id):
    url = "https://api.edunet.net/search/searchApi/search"
    
    params = {
        # [í•µì‹¬ ìˆ˜ì •] ì»¬ë ‰ì…˜ IDë¥¼ ë³€ìˆ˜(collection_id)ë¡œ ë°›ìŠµë‹ˆë‹¤.
        "collection": collection_id, 
        "sort": "r",            
        "searchType": "all",
        "pageNum": 1,
        "pageSize": 10,
        "sno": SNO_KEY,         
        "svc_version": "4.5",   
        "svc_domain": SVC_DOMAIN 
    }
    
    encoded_query = urllib.parse.quote(query, encoding='utf-8')
    full_url = f"{url}?kwd={encoded_query}&" + urllib.parse.urlencode(params)
    
    try:
        response = requests.get(full_url, timeout=10) 
        
        if response.status_code == 200 and 'xml' in response.headers.get('Content-Type', '').lower():
            return xmltodict.parse(response.content.decode('utf-8'))
        else:
            return {"error": f"HTTP ì˜¤ë¥˜ ë°œìƒ: {response.status_code}", "raw_content": response.text}
            
    except requests.exceptions.RequestException as e:
        return {"error": f"API í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}"}

# --- UI ë° ê²€ìƒ‰ ì‹¤í–‰ ---
search_query = st.text_input("ê¶ê¸ˆí•œ ê³¼ëª©/í‚¤ì›Œë“œ (ì˜ˆ: ê²½ì œìˆ˜í•™, ì¸ê³µì§€ëŠ¥ ê¸°ì´ˆ)", "ê²½ì œìˆ˜í•™")

# [ì¶”ê°€] ì»¬ë ‰ì…˜ ì„ íƒ ê¸°ëŠ¥ (í•„í„°)
collection_options = {
    "ğŸ” ì „ì²´ ê²€ìƒ‰ (ê°€ì¥ ë„“ì€ ë²”ìœ„)": "total",
    "ğŸ“š ê³ êµí•™ì ì œ ê´€ë ¨ ìë£Œë§Œ": "cre_sys",
    "ğŸ“ êµìœ¡ê³¼ì • ë° ìˆ˜ì—… ìë£Œ": "crclm,lsn_design"
}
selected_option = st.selectbox("ê²€ìƒ‰ ëŒ€ìƒ ì»¬ë ‰ì…˜ ì„ íƒ", list(collection_options.keys()))
collection_id = collection_options[selected_option]


if search_query:
    with st.spinner(f"ì—ë“€ë„· ê³µì‹ ìë£Œë¥¼ '{search_query}'ë¡œ ê²€ìƒ‰ ì¤‘ (ëŒ€ìƒ: {selected_option})..."):
        api_result = search_keris_contents(search_query, collection_id)
    
    if "error" in api_result:
        st.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {api_result['error']}")
        with st.expander("ì›ë³¸ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸"):
            st.write(api_result.get("raw_content", "ë¡œê·¸ ì—†ìŒ"))
    
    else:
        try:
            total_count = int(api_result['search']['totalCount'])
            st.success(f"âœ… ì´ {total_count}ê±´ì˜ ê³µì‹ ìë£Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            if total_count > 0:
                data_list_node = api_result['search']['totalResults']['dataList']
                
                if data_list_node is not None and 'data' in data_list_node:
                    data_list = data_list_node['data']
                else:
                    data_list = [] 
                
                if not isinstance(data_list, list):
                    data_list = [data_list]

                if data_list:
                    def remove_html_tags(text):
                        if text is None:
                            return ''
                        return re.sub(r'</?b>', '', str(text))

                    for i, item in enumerate(data_list):
                        clean_title = remove_html_tags(item.get('ttl', 'ì œëª© ì—†ìŒ')) 
                        clean_category = remove_html_tags(item.get('srvc_clsf_nm_path', 'ë¶„ë¥˜ ì •ë³´ ì—†ìŒ'))
                        clean_summary = remove_html_tags(item.get('cn', 'ìƒì„¸ ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.'))
                        link = item.get('conts_link')
                        
                        st.markdown(f"### {i+1}. {clean_title}")
                        st.caption(f"ë¶„ë¥˜: {clean_category}")
                        st.write(clean_summary)
                        
                        if link:
                            st.markdown(f"[ğŸ”— ì—ë“€ë„· ìƒì„¸ ìë£Œ ë°”ë¡œê°€ê¸°]({link})")
                        st.markdown("---")
            else:
                st.warning(f"'{search_query}' ê´€ë ¨ ìë£Œê°€ ì—ë“€ë„·ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œ(ì˜ˆ: ìˆ˜í•™ êµìœ¡ê³¼ì •)ë¥¼ ì‹œë„í•´ ë³´ì„¸ìš”.")
                
        except KeyError as e:
            st.error(f"âš ï¸ ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜: í•„ìˆ˜ í•„ë“œ '{e}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            with st.expander("ì›ë³¸ ë¡œê·¸ í™•ì¸"):
                st.json(api_result)
        except Exception as e:
             st.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

st.divider()
st.caption("ì œê³µ: í•œêµ­êµìœ¡í•™ìˆ ì •ë³´ì›(KERIS) ì—ë“€ë„·Â·í‹°í´ë¦¬ì–´ API ê¸°ë°˜")
